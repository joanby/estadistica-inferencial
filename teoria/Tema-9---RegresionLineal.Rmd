---
title: "Tema 8 - Regresión Lineal"
author: "Ricardo Alberich, Juan Gabriel Gomila y Arnau Mir"
date: 
output: 
  ioslides_presentation:
    widescreen: true
    css: Mery_style.css
    logo: Images/matriz_mov.gif
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Regresión lineal simple

## Introducción

El problema de **regresión** consiste en hallar la mejor **relación funcional** entre dos variables $X$ e $Y$.

Más concretamente, dada una muestra de las dos variables $X,Y$, $(x_i,y_i)_{i=1,2,\ldots,n}$, queremos estudiar cómo depende el valor de $Y$ en función del valor de $X$.

La variable aleatoria $Y$ es la variable **dependiente** o **de respuesta**.

La variable (no necesariamente aleatoria) $X$ es la variable **de control**,
**independiente** o **de regresión**. Pensemos por ejemplo, en un experimento donde la variable $X$ es la que controla el experimentador y la variable $Y$ es el valor que se obtiene del experimento.

## Introducción

El problema de **regresión** es encontrar la mejor **relación funcional** que explique la variable $Y$ conocidas las observaciones de la
variable $X$. 

Si dicha **relación funcional** es una recta, $Y=\beta_0 +\beta_1 x$, la **regresión** se denomina **regresión lineal**.

En la **regresión lineal**, se hace la suposición siguiente:
$$
\mu_{Y|x}=\beta_0+\beta_1 x,
$$
dónde $\mu_{Y|x}$ es el valor esperado de la variable aleatoria $Y$ cuando la variable $X$ vale $x$. Dicho valor esperado es una función lineal de $X$ con **término independiente** $\beta_0$ y **pendiente** $\beta_1$. Dichos valores son dos
parámetros que tendremos que estimar.


## Introducción

Las estimaciones de $\beta_0$ y $\beta_1$ se llaman $b_0$ y $b_1$, respectivamente y se tienen que realizar a partir de la muestra $(x_i,y_i)_{i=1,2,\ldots,n}$.

Una vez halladas las estimaciones $b_0$ y $b_1$, obtendremos la **recta de regresión** para nuestra muestra:
$$
\widehat{y}=b_0+b_1 x,
$$
que dado un valor $x_0$ de $X$, estimará el valor $\widehat{y}_0=b_0+b_1 x_0$ de la variable $Y$.

## Mínimos cuadrados
Vamos a explicar el método para hallar las estimaciones $b_0$ y $b_1$. 

Dicho método se denomina **método de los mínimos cuadrados**.

Dada una observación cualquiera de la muestra, $(x_i,y_i)$, podremos separar la componente $y_i$ como la suma de su **valor predicho por el modelo** y el error cometido:
$$
y_i=\beta_0+\beta_1 x_i+ \varepsilon_i\Rightarrow \varepsilon_i=y_i-(\beta_0+\beta_1 x_i).
$$
Llamamos  **error cuadrático teórico** de este modelo a la suma al cuadrado de todos los errores cometidos por los valores de la muestra:
$$
SS_\varepsilon=\sum_{i=1}^n \varepsilon_i^2=\sum_{i=1}^n (y_i-\beta_0-\beta_1 x_i)^2.
$$

## Mínimos cuadrados


La **regresión lineal por mínimos cuadrados** consiste en hallar los estimadores $b_0$ y  $b_1$ de $\beta_0$ y $\beta_1$ que minimicen dicho **error cuadrático teórico** $SS_\varepsilon$.

<l class="observ">Observación: </l> los errores cometidos pueden ser positivos o negativos. Entonces, para asegurarse de penalizar siempre los errores, se elevan éstos al cuadrado y de esta forma, siempre se suman y no pueden anularse.

Para hallar el mínimo del **error cuadrático teórico**, $(b_0,b_1)$, hay que derivar respecto las variables $\beta_0$ y $\beta_1$ e igualar a $0$ dichas derivadas:
$$
\begin{array}{ll}
\dfrac{\partial SS_\varepsilon}{\partial \beta_0}_{|\beta_0=b_0,\beta_1=b_1}=&-2\sum\limits_{i=1}^n (y_i -b_0-b_1 x_i)=0,\\[1ex]
\dfrac{\partial SS_\varepsilon}{\partial \beta_1}_{|\beta_0=b_0,\beta_1=b_1}=&-2\sum\limits_{i=1}^n (y_i -b_0-b_1 x_i) x_i =0.
\end{array}
$$

## Mínimos cuadrados
La solución del sistema anterior es:
$$
\begin{array}{rl}
b_1& \displaystyle=\frac{n \sum\limits_{i=1}^n x_i y_i-\sum\limits_{i=1}^n x_i\sum\limits_{i=1}^n y_i} {n\sum\limits_{i=1}^n
x_i^2-(\sum\limits_{i=1}^n x_i)^2},\\
b_0& \displaystyle=\frac{\sum\limits_{i=1}^n y_i -b_1 \sum\limits_{i=1}^n x_i}{n}.
\end{array}
$$

## Mínimos cuadrados
Vamos a escribir las estimaciones $b_0$ y $b_1$ obtenidas anteriormente en función de las medias, varianzas y covarianza de la muestra de valores $(x_i,y_i)$.

Sean entonces 
$$
\overline{x}=\frac1{n}\sum\limits_{i=1}^n x_i,
\quad \overline{y}=\frac1{n} \sum\limits_{i=1}^n y_i,
$$
las medias de las componentes $x$ e $y$ de los valores de la muestra y sean

## Mínimos cuadrados
$$
\begin{array}{rl}
\tilde{s}_x^2 &\displaystyle =\frac1{n-1}\sum_{i=1}^n (x_i-\overline{x})^2 =\frac{n}{n-1}\left(\frac1{n}\Big(\sum_{i=1}^n x_i^2\Big) -\overline{x}^2\right),\\
\tilde{s}_y^2 &\displaystyle =\frac1{n-1}\sum_{i=1}^n (y_i-\overline{y})^2 =\frac{n}{n-1}\left(\frac1{n}\Big(\sum_{i=1}^n y_i^2\Big) -\overline{y}^2\right),\\
\tilde{s}_{xy} &\displaystyle =\frac1{n-1}\sum_{i=1}^n (x_i-\overline{x}) (y_i-\overline{y}) =\frac{n}{n-1}\left(\frac1{n}\Big(\sum_{i=1}^n x_i y_i\Big)-\overline{x}\cdot\overline{y}\right),
\end{array}
$$
las varianzas y la covarianza de la muestra $(x_i,y_i)$, $i=1,\ldots,n$.

## Mínimos cuadrados
Las estimaciones $b_0$ y $b_1$ se pueden reescribir de la forma siguiente:

<l class="prop">Teorema. </l>
Los estimadores $b_0$ y $b_1$ de  $\beta_0$ y $\beta_1$, respectivamente, hallados por el **método de los mínimos cuadradados** son los siguientes: $b_1 =\frac{\tilde{s}_{xy}}{\tilde{s}_x^2},\quad b_0 = \overline{y}-b_1 \overline{x}.$

<div class="exercise">
**Ejercicio**

Se deja como ejercicio la demostración del teorema anterior a partir de la expresión hallada anteriormente.

</div>

Dado un valor $x$ de la variable $X$, llamaremos $\widehat{y}$ a la expresión $\widehat{y}=b_0+b_1x$ al **valor estimado** de $Y$  cuando $X=x$.

Dada una observación $(x_i,y_i)$, llamaremos **error** de la observación $e_i$ a la expresión $e_i=y_i-\widehat{y}_i={y}_i-b_0-b_1x_i$.


## Ejemplo
<div class="example">
**Ejemplo**

En un experimento donde se quería estudiar la asociación entre consumo de sal y presión arterial, se asignó aleatoriamente a algunos individuos una cantidad diaria constante de sal en su dieta, y al cabo de un mes se los midió la tensión arterial media. Algunos resultados fueron los siguientes:

<div class="center">
|$X$ (sal, en g) | $Y$ (Presión, en mm de Hg)|
|:---:|---:|
|1.8 |  100|
|2.2 |  98|
|3.5 |  110|
|4.0 |  110|
|4.3 |  112|
|5.0 |  120|
</div>
</div>


## Ejemplo
<div class="example">
Vamos a hallar la **recta de regresión lineal por mínimos cuadrados** de $Y$ en función de $X$.

En primer lugar calculamos las medias, varianzas y covarianza de la muestra de datos:
```{r}
sal=c(1.8,2.2,3.5,4.0,4.3,5.0)
tensión=c(100,98,110,110,112,120)
(media.sal = mean(sal))
(media.tensión = mean(tensión))
```

</div>


## Ejemplo
<div class="example">
```{r}
(var.sal = var(sal))
(var.tensión = var(tensión))
(cov.sal.tensión = cov(sal,tensión))
```


</div>

## Ejemplo
<div class="example">
Los estimadores $b_0$ y $b_1$ serán:
```{r}
(b1 = cov.sal.tensión/var.sal)
(b0 = media.tensión-b1*media.sal)
```
La recta de regresión será, en este caso: $\widehat{y} = `r b0`+`r b1`\cdot x$.

</div>

## Recta de regresión en `R`
Para hallar la recta de regresión en `R` hay que usar la función `lm`:
```{r}
lm(tensión ~sal)
```

## Propiedades de la recta de regresión
La **recta de regresión** hallada por el **método de los mínimos cuadrados** verifica las propiedades siguientes:

* La **recta de regresión** pasa por el vector medio $(\overline{x},\overline{y})$ de nuestra muestra de datos $(x_i,y_i)$, $i=1,\ldots,n$:
$$
\overline{y}=b_0+b_1 \overline{x}.
$$

* La media de los valores estimados a partir de la **recta de regresión** es igual a la media de los
observados $y_i$, $\overline{y}$. Es decir:
$$
\overline{\widehat{y}}=\frac1{n}\sum_{i=1}^n\widehat{y}_i =\frac1{n}\sum_{i=1}^n(b_0+b_1x_i)=
b_0+b_1 \overline{x}=\overline{y}.
$$


## Propiedades de la recta de regresión

* Los errores $(e_i)_{i=1,\ldots,n}$ tienen media 0:
$$
\overline{e}
=\frac1{n}\sum_{i=1}^n e_i =\frac1{n}\sum_{i=1}^n (y_i-b_0-b_1x) =\frac1{n}\sum_{i=1}^n (y_i-\widehat{y}_i) =0.
$$


Llamaremos **suma de cuadrados de los errores** a la cantidad siguiente: $\displaystyle SS_E=\sum_{i=1}^{n} e^2_i.$

Usando que los errores $(e_i)_{i=1,\ldots,n}$ tienen media $0$, su varianza será:
$$
s_e^2=\frac1{n}\Big(\sum_{i=1}^{n}
e^2_i\Big)-\overline{e}^2=\frac{SS_E}{n}-0=\frac{SS_E}{n}.
$$

## Propiedades de la recta de regresión
Definimos las variables aleatorias $E_{x_i}$ como $E_{x_i}=y_i-b_0-b_1\cdot x_i$ donde $(x_i,y_i)$ es un valor de la muestra y $b_0$ y $b_1$ son los estimadores obtenidos por el **método de los mínimos cuadrados**. Entonces, 

<l class="prop"> Teorema.</l>
Si las variables aleatorias error $E_{x_i}$ tienen todas media 0 y la misma varianza $\sigma^2_E$ y, dos a dos, tienen covarianza 0, entonces

* $b_0$ y son $b_1$ los estimadores lineales no sesgados óptimos (más eficientes) de  $\beta_0$ y $\beta_1$, respectivamente.

y un **estimador no sesgado de $\sigma_E^2$** es el siguiente: $S^2=\frac{SS_E}{n-2}$.

Si, además, las variables aleatorias error $E_{x_i}$ son **normales**, entonces
 $b_0$ y son $b_1$ los estimadores máximo verosímiles de  $\beta_0$ y $\beta_1$, respectivamente.

## Ejemplo
<div class="example">
Comprobemos las propiedades para los datos del ejemplo anterior:

* La **recta de regresión** pasa por el vector medio $(\overline{x},\overline{y})$:
```{r}
(round(media.tensión-b0-b1*media.sal,6))
```

* La media de los valores estimados a partir de la **recta de regresión** es igual a la media de los
observados $y_i$, $\overline{y}$.
```{r}
tensión.estimada = b0+b1*sal
(mean(tensión.estimada)-mean(tensión))
```

</div>

## Ejemplo
<div class="example">
La estimación de la varianza para los datos del ejemplo anterior es la siguiente:
```{r}
errores=tensión.estimada-tensión
SSE = sum(errores^2)
n=length(sal)
(estimación.varianza = SSE/(n-2))
```
Entonces tenemos que el valor aproximado o estimado de $\sigma_E^2$ es `r estimación.varianza`.
</div>

## Coeficiente de determinación
Llegados a este punto, nos preguntamos lo efectiva que es la **recta de regresión**.

Es decir, cómo medir si la aproximación hallada $\widehat{y}=b_0+b_1 x$ a la nube de puntos $(x_i,y_i),\ i=1,\ldots,n$ ha sido suficientemente buena.

Una forma de realizar dicha medición es a través del **coeficiente de determinación** $R^2$ que estima cuánta **variabilidad** de los valores $y_i$ heredan los valores estimados $\widehat{y}_i$.

Para ver su definición, necesitamos introducir las **variabilidades** siguientes:

## Coeficiente de determinación

* **Variabilidad total** o suma total de cuadrados: $SS_T =\sum\limits_{i=1}^n(y_i-\overline{y})^2 = (n-1)\cdot \tilde{s}_y^2.$

* **Variabilidad de la regresión** o suma de cuadrados de la regresión: $SS_R=\sum\limits_{i=1}^n(\widehat{y}_i-\overline{y})^2=(n-1)\cdot \tilde{s}_{\widehat{y}}^2.$

* **Variabilidad del error** o suma de cuadrados del error: $SS_E=\sum\limits_{i=1}^n(y_i-\widehat{y}_i)^2=(n-1)\cdot \tilde{s}_e^2.$

El teorema siguiente nos relaciona las variabilidades anteriores:

## Coeficiente de determinación
<l class="prop">Teorema. </l>
En una regresión lineal usando el método de los mínimos cuadrados, se cumple la siguiente relación entre las **variabilidades**:
$$
SS_T=SS_R+SS_E,
$$
o equivalentemente,
$$
\tilde{s}^2_y=\tilde{s}^2_{\widehat{y}}+\tilde{s}^2_e.
$$

Entonces, cuántas más "proximas" estén las **variabilidades** $SS_T$ y $SS_R$, o, si se quiere, $\tilde{s}^2_y$ y $\tilde{s}^2_{\widehat{y}}$, más efectiva habrá sida la regresión, ya que la regresión habrá heredado mucha variabilidad de los datos $y_i$, $i=1,\ldots,n$ y la variabilidad del error, $SS_E$ será pequeña.

## Coeficiente de determinación
El comentario anterior motiva la definición siguiente del **coeficiente de determinación** para medir la efectividad de la recta de regresión:

<l class="definition"> Definición: </l>
se define el **coeficiente de determinación** $R^2$ en la regresión por el método de los mínimos cuadrados como: $R^2 = \frac{SS_R}{SS_T}=\frac{\tilde{s}_{\widehat{y}}^2}{\tilde{s}_y^2}.$

<l class="observ"> Observación: </l> el **coeficiente de determinación** $R^2$ es la fracción de la variabilidad de las componentes $y$ que queda explicada por la variabilidad de las estimaciones correspondientes $\widehat{y}$.

## Propiedades del coeficiente de determinación
* El **coeficiente de determinación** es una cantidad entre 0 y 1: $0\leq R^2\leq 1$. Entonces, cuánto más próximo a 1 esté dicho coeficiente, más precisa será la recta de regresión.

* El **coeficiente de determinación** se puede expresar en función de la **variabilidad del error** de la forma siguiente:
$$
R^2=\frac{SS_T-SS_E}{SS_T}=1-\frac{SS_E}{SS_T}=1-\frac{\tilde{s}_e^2}{\tilde{s}_y^2}.
$$

* Se define el **coeficiente de correlación lineal** $r_{xy}$ como $r_{xy}=\frac{\tilde{s}_{xy}}{\tilde{s}_x\cdot \tilde{s}_y}$. Entonces, el **coeficiente de determinación** $R^2$ es el cuadrado del **coeficiente de correlación lineal**: $R^2 = r_{xy}^2$.

## Propiedades del coeficiente de determinación
Veamos la demostración de la última propiedad:
$$
\begin{array}{rl}
R^2 & \displaystyle =\frac{SS_R}{SS_T}=\frac{\sum\limits_{i=1}^n(b_1x_i+b_0-\overline{y})^2}{(n-1)\tilde{s}_y^2} =\frac{\sum\limits_{i=1}^n\left(\dfrac{\tilde{s}_{xy}}{\tilde{s}_x^2}x_i-\dfrac{\tilde{s}_{xy}}{\tilde{s}_x^2}\overline{x}\right)^2}{(n-1)\tilde{s}_y^2}\\
& \displaystyle =\frac{\dfrac{\tilde{s}_{xy}^2}{\tilde{s}_x^4}\sum\limits_{i=1}^n(x_i-\overline{x})^2}{(n-1)\tilde{s}_y^2} =\dfrac{\tilde{s}_{xy}^2}{\tilde{s}_x^4}\cdot \frac{\tilde{s}_x^2}{\tilde{s}_y^2}=\frac{\tilde{s}_{xy}^2}{\tilde{s}_x^2\cdot \tilde{s}_y^2}=r_{xy}^2
\end{array}
$$

## Ejemplo
<div class="example">
Calculemos las variabilidades anteriores y el **coeficiente de determinación** para los datos de nuestro ejemplo:

* **Variabilidad total**:
```{r}
(SST = sum((tensión-media.tensión)^2))
```
* **Variabilidad de la regresión**:
```{r}
(SSR = sum((tensión.estimada-media.tensión)^2))
```
</div>

## Ejemplo
<div class="example">
* **Variabilidad del error**:
```{r}
(SSE = sum((tensión-tensión.estimada)^2))
```
Comprobemos que se cumple $SST=SSR+SSE$:
```{r}
(round(SST-SSR-SSE,6))
```
</div>

## Ejemplo
<div class="example">
El coeficiente de determinación $R^2$ será:
```{r}
(R2=SSR/SST)
```
Otra manera de calcularlo es:
```{r}
(R2=var(tensión.estimada)/var(tensión))
```
En este caso, la regresión explica un `r round(100*R2,2)`% de la variabilidad de los datos.
</div>

## Coeficiente de determinación en `R`
Para hallar el **coeficiente de determinación** en `R` hemos de usar las funciones `lm` y `summary` junto con el parámetro `r.squared`:

```{r,eval=FALSE}
summary(lm(y ~ x))$r.squared
```

<div class="example">
Si aplicamos las funciones anteriores a los datos de nuestro ejemplo, obtenemos:
```{r}
summary(lm(tensión ~ sal))$r.squared
```
</div>

## Coeficiente de determinación
Usar solamente el **coeficiente de determinación** para medir la calidad de la regresión es un error. 

Tenemos que observar más información para poder afirmar que la regresión obtenida es adecuada y se ajusta bien a nuestros datos.

En `R` existe una tabla de datos denominada `anscombe` que pone de manifiesto este hecho.
Vamos a echarle un vistazo:

## Coeficiente de determinación
```{r}
data(anscombe)
str(anscombe)
```
Vemos que tiene 4 parejas de valores $(x_i,y_i)$ de tamaño 11:


## Coeficiente de determinación
```{r}
anscombe
```

## Coeficiente de determinación
Si calculamos los **coeficientes de determinación** para las 4 parejas, obtenemos un resultado similar:
```{r}
summary(lm(y1~x1,data=anscombe))$r.squared
summary(lm(y2~x2,data=anscombe))$r.squared
```


## Coeficiente de determinación
```{r}
summary(lm(y3~x3,data=anscombe))$r.squared
summary(lm(y4~x4,data=anscombe))$r.squared
```

En cambio, si vemos su representación gráfica, su aspecto es muy distinto:

## Coeficiente de determinación
```{r,fig.align='center',eval=FALSE}
par(mfrow=c(2,2))
plot(y1~x1,data=anscombe)
abline(lm(y1~x1,data=anscombe),col=2)
plot(y2~x2,data=anscombe)
abline(lm(y2~x2,data=anscombe),col=2)
plot(y3~x3,data=anscombe)
abline(lm(y3~x3,data=anscombe),col=2)
plot(y4~x4,data=anscombe)
abline(lm(y4~x4,data=anscombe),col=2)
```

## Coeficiente de determinación
```{r,fig.align='center',echo=FALSE}
par(mfrow=c(2,2))
plot(y1~x1,data=anscombe)
abline(lm(y1~x1,data=anscombe),col=2)
plot(y2~x2,data=anscombe)
abline(lm(y2~x2,data=anscombe),col=2)
plot(y3~x3,data=anscombe)
abline(lm(y3~x3,data=anscombe),col=2)
plot(y4~x4,data=anscombe)
abline(lm(y4~x4,data=anscombe),col=2)
```


## Coeficiente de determinación
Observamos que en el caso de la tabla de datos `(x3,y3)`, la recta de regresión ha sido efectiva pero en los demás hemos obtenido un error considerable donde el peor caso es el de la tabla de datos `(x4,y4)`.

Por tanto, considerar sólo el valor del **coeficiente de determinación** para medir el ajuste de la recta de regresión a nuestros datos no es conveniente.



## Intervalos de confianza
Para poder hallar los intervalos de confianza al $100\cdot (1-\alpha)$% de confianza sobre los parámetros $\beta_0$ y $\beta_1$, necesitamos el supuestos siguiente:

Para cada valor $x_i$ de la variable $X$, las variables aleatorias $E_{x_i}$ sigue una distribución normal con media $\mu_{E_{x_i}}=0$ y varianza $\sigma_E^2$ constante independiente del valor $x_i$. También supondremos que dados $x_i$ y $x_j$ dos valores distintos de la variable $X$, la covarianza entre las variables $E_{x_i}$ y $E_{x_j}$ es nula: $\sigma(E_{x_i},E_{x_j})=0$.


## Ejemplo
<div class="example">
Comprobemos para los datos de nuestro ejemplo si los errores siguen una distribución normal usando el test de **Kolmogorov-Smirnov-Lilliefors**:
```{r}
library(nortest)
lillie.test(errores)
```
Como el p-valor es grande, podemos concluir que no tenemos evidencias suficientes para rechazar que los errores siguen una distribución normal.


</div>



## Intervalos de confianza
Bajo las hipótesis anteriores tenemos los dos resultados siguientes:

<l class="prop">Teorema. </l>
Los errores estándar de los estimadores $b_1$ y $b_0$ son, respectivamente,
$$
\frac{\sigma_E}{\tilde{s}_x\sqrt{n-1}}\quad\mbox{ y }\quad \sigma_E\sqrt{\frac{1}{n}+\frac{\overline{x}^2}{(n-1)\tilde{s}_x^2}},
$$
donde recordemos que para estimar $\sigma_E$, usamos el estimador $S=\sqrt{S^2}$.

## Intervalos de confianza
<l class="prop">Teorema. </l>
Las variables aleatorias
$$
\frac{b_1-\beta_1}{\frac{S}{\tilde{s}_x\sqrt{n-1}}}\quad\mbox{ y }\quad
\frac{b_0-\beta_0}{S\sqrt{\frac{1}{n}+\frac{\overline{x}^2}{(n-1)\tilde{s}_x^2}}},
$$
siguen leyes $t$ de Student con $n-2$ grados de libertad.

## Intervalos de confianza
Entonces bajo el supuesto anterior, los intervalos de confianza para los parámetros $\beta_1$ y $\beta_0$ al $100\cdot (1-\alpha)$% de confianza son los siguientes:

* $\beta_1$: $\left] b_1-t_{n-2,1-\frac{\alpha}2} \frac{S}{\tilde{s}_x\sqrt{n-1}}, b_1+t_{n-2,1-\frac{\alpha}2} \frac{S}{\tilde{s}_x\sqrt{n-1}}\right[.$
Lo escribiremos para simplificar de la forma siguiente: $\beta_1=b_1\pm t_{n-2,1-\frac{\alpha}2} \frac{S}{\tilde{s}_x\sqrt{n-1}}$.

* $\beta_0$: $b_0\pm t_{n-2,1-\frac{\alpha}2}S\sqrt{\frac{1}{n}+\frac{\overline{x}^2}{(n-1)\tilde{s}_x^2}}$.

## Ejemplo
<div class="example">
Hallemos los intervalos de confianza para el 95% de confianza para los parámetros $\beta_1$ y $\beta_0$ usando los datos del ejemplo que hemos ido desarrollando:
```{r}
alpha=0.05
S=sqrt(estimación.varianza)
extremo.izquierda.b1 = b1-qt(1-alpha/2,n-2)*S/(sd(sal)*sqrt(n-1))
extremo.derecha.b1 = b1+qt(1-alpha/2,n-2)*S/(sd(sal)*sqrt(n-1))
print('Intervalo confianza para b1:')
print(c(extremo.izquierda.b1,extremo.derecha.b1))
```

</div>

## Ejemplo
<div class="example">
```{r}
extremo.izquierda.b0 = b0-qt(1-alpha/2,n-2)*S*sqrt(1/n+media.sal^2/((n-1)*var(sal)))
extremo.derecha.b0 = b0+qt(1-alpha/2,n-2)*S*sqrt(1/n+media.sal^2/((n-1)*var(sal)))
print('Intervalo confianza para b0:')
print(c(extremo.izquierda.b0,extremo.derecha.b0))
```

</div>

## Intervalos de confianza en `R`
Para hallar los intervalos de confianza de los parámetros $\beta_1$ y $\beta_0$ en `R` hay que aplicar la función `confint` al objeto `lm(...)`. El parámetro `level` nos da el nivel de confianza cuyo valor por defecto es 0.95.

<div class="example">
Para los datos de nuestro ejemplo, los intervalos de confianza para los parámetros $\beta_0$ y $\beta_1$ serían los siguientes en `R`:
```{r}
confint(lm(tensión~sal),level=0.95)
```

</div>


## Intervalos de confianza
Fijado un valor $x_0$ de la variable $X$, podemos considerar dos parámetros a estudiar: el valor medio de la variable aleatoria $Y|x_0$, $\mu_{Y|x_0}$ y el valor estimado $y_0$ por la recta de regresión.

Dichos intervalos nos ayudan a estudiar cómo se comporta la regresión cuando el valor de la variable $X$ vale un determinado valor $x_0$.

El estimador de los parámetros anteriores, $\mu_{Y|x_0}$ y $y_0$ es el mismo: $\widehat{y}_0 = b_0+b_1\cdot x_0$ pero los errores estándards cambian dependiendo del parámetro que consideremos como indican los dos resultados siguientes:

## Intervalos de confianza
Bajo el supuesto anterior, sea $x_0$ un valor concreto de la variable $X$. Entonces:

<l class="prop">Teorema. </l>
El error estándar del estimador $\widehat{y}_0$ del parámetro $\mu_{Y|x_0}$ es el siguiente: $\sigma_E\sqrt{\frac1{n}+\frac{(x_0-\overline{x})^2}{(n-1)\tilde{s}^2_x}}.$

Usando el resultado anterior, se tiene que la variable aleatoria: $\frac{\widehat{y}_0-\mu_{Y/x_0}}{S\sqrt{\frac1{n}+\frac{(x_0-\overline{x})^2}{(n-1) \tilde{s}^2_x}}}$
sigue una ley $t$ de Student con $n-2$ grados de libertad.


## Intervalos de confianza

<l class="prop">Teorema. </l>
El error estándar del estimador $\widehat{y}_0$ del parámetro $y_0$ es el siguiente: $\sigma_E\sqrt{1+\frac1{n}+\frac{(x_0-\overline{x})^2}{(n-1)\tilde{s}^2_x}}.$

Usando el resultado anterior, se tiene que la variable aleatoria: $\frac{\widehat{y}_0-y_0}{S\sqrt{1+\frac1{n}+\frac{(x_0-\overline{x})^2}{(n-1) \tilde{s}^2_x}}}$
sigue una ley $t$ de Student con $n-2$ grados de libertad.

## Intervalos de confianza
A partir de los teoremas anteriores, hallamos los intervalos de confianza para los parámetros $\mu_{Y|x_0}$ y $y_0$ al $100\cdot (1-\alpha)$% de confianza:

* $\mu_{Y|x_0}$: $\widehat{y}_0\pm t_{n-2,1-\frac{\alpha}2} S\sqrt{\frac1{n}+\frac{(x_0-\overline{x})^2}{(n-1)
\tilde{s}^2_x}}$.

* $y_0$: $\widehat{y}_0\pm t_{n-2,1-\frac{\alpha}2} S\sqrt{1+\frac1{n}+\frac{(x_0-\overline{x})^2}{(n-1)
\tilde{s}^2_x}}$.

## Ejemplo
<div class="example">
Hallemos un intervalo de confianza para un nivel de sal de $x_0=4.5$ g. para los parámetros $\mu_{Y|4.5}$ y $y_0$ al $95$% de confianza:
```{r}
alpha=0.05
x0=4.5
y0.estimado = b0+b1*x0
extremo.izquierda.mu.x0 = y0.estimado-qt(1-alpha/2,n-2)*S*
  sqrt(1/n+(x0-media.sal)^2/((n-1)*var(sal)))
extremo.derecha.mu.x0 = y0.estimado+qt(1-alpha/2,n-2)*S*
  sqrt(1/n+(x0-media.sal)^2/((n-1)*var(sal)))
print(paste("Intervalo de confianza para mu de Y para x0=",x0))
print(c(extremo.izquierda.mu.x0,extremo.derecha.mu.x0))
```

</div>


## Ejemplo
<div class="example">
```{r}
extremo.izquierda.y0 = y0.estimado-qt(1-alpha/2,n-2)*S*
  sqrt(1+1/n+(x0-media.sal)^2/((n-1)*var(sal)))
extremo.derecha.y0 = y0.estimado+qt(1-alpha/2,n-2)*S*
  sqrt(1+1/n+(x0-media.sal)^2/((n-1)*var(sal)))
print(paste("Intervalo de confianza para y0 para x0=",x0))
print(c(extremo.izquierda.y0,extremo.derecha.y0))
```


</div>

## Intervalos de confianza en `R`
Para hallar el intervalo de confianza para el parámetros $\mu_{Y|x_0}$ al $100\cdot (1-\alpha)$% de confianza hay que usar la función `predict.lm` de la forma siguiente:
```{r,eval=FALSE}
newdata=data.frame(x=x0)
predict.lm(lm(y~x),newdata,interval="confidence",level= nivel.confianza)
```

Para el parámetro $y_0$ hay que usar la misma función anterior pero cambiando el parámetro `interval` al valor `prediction`:

```{r,eval=FALSE}
newdata=data.frame(x=x0)
predict.lm(lm(y~x),newdata,interval="prediction",level= nivel.confianza)
```


## Ejemplo
<div class="example">
Hallemos los intervalos de confianza para los parámetros $\mu_{Y|4.5}$ y $y_0$ al 95% de confianza usando `R`:

```{r}
newdata=data.frame(sal=4.5)
predict.lm(lm(tensión~sal),newdata,interval="confidence",level= 0.95)
predict.lm(lm(tensión~sal),newdata,interval="prediction",level= 0.95)
```

</div>

## Contraste de hipótesis sobre la pendiente de la recta $\beta_1$
Cuando nos planteamos hacer una regresión de la variable $Y$ sobre la variable $X$, estamos suponiendo que la variable $X$ influye en la variable $Y$. Es decir, que si cambiamos el valor de la variable $X$, habrá un cambio en la variable $Y$.

Decir que la variable $X$ influye en la variable $Y$ en el contexto de la **regresión lineal** es equivalente a decir que $\beta_1\neq 0$ ya que si $\beta_1=0$, tendríamos que la variación de la variable $Y$ sólo se debería a fluctuaciones aleatorias.


## Contraste de hipótesis sobre la pendiente de la recta $\beta_1$

Por tanto, es interesante plantearse el **contraste de hipótesis** siguiente sobre el parámetro **pendiente de la recta de regresión**:
$\beta_1$:
$$
\left\{\begin{array}{l}
H_0:\beta_1=0,\\
H_1:\beta_1 \neq 0.
\end{array}
\right.
$$
En el supuesto de que las variables aleatorias $E_{x_i}$ son normales $N(0,\sigma_E^2)$, el **estadístico de contraste** para realizar el contraste anterior es el siguiente:
$T=\frac{b_1}{\frac{S}{\tilde{s}_x \sqrt{n-1}}},$
que, suponiendo que la hipótesis nula es cierta, se distribuye según una t de Student con $n-2$ grados de libertad.

## Contraste de hipótesis sobre la pendiente de la recta $\beta_1$
Si $t_0$ es el valor del estadístico de contraste para los valores de nuestra muestra, el p-valor del contraste anterior es el siguiente:
$$
p=2\cdot P(t_{n-2}>|t_0|),
$$
con el significado usual:

## Contraste de hipótesis sobre la pendiente de la recta $\beta_1$

* si $p<0.05$, concluimos que tenemos evidencias suficientes para rechazar la hipótesis nula y, por tanto, tiene sentido la regresión al no rechazar que $\beta_1\neq 0$,
* si $p>0.1$, concluimos que no tenemos evidencias suficientes para rechazar la hipótesis nula. En este caso, la regresión no tendría sentido al no rechazar que $\beta_1=0$ y,
* si $0.05\leq p\leq 0.1$, estamos en la zona de penumbra. Necesitamos más datos para tomar una decisión clara.

## Contraste de hipótesis sobre la pendiente de la recta $\beta_1$
Otra forma de realizar el contraste anterior es observar el intervalo de confianza para $\beta_1$ y ver si contiene el valor 0.

En caso de contener el valor 0, la regresión no tendría sentido para el nivel de confianza de $100\cdot (1-\alpha)$% ya que no rechazamos que $\beta_1=0$ y en caso de no contener el valor 0, la regresión sí tendría sentido al nivel de confianza anterior.

## Ejemplo
<div class="example">
Realicemos el contraste anterior para los datos de nuestro ejemplo. El valor del estadístico de contraste será:
```{r}
(t0 = b1/(S/(sd(sal)*sqrt(n-1))))
```
y el p-valor vale:
```{r}
(p=2*pt(abs(t0),n-2,lower.tail = FALSE))
```
Como el p-valor es pequeño, podemos concluir que la regresión tiene sentido en este caso.


</div>

## Contraste para el pendiente $\beta_1$ en `R`
Para realizar el contraste anterior en `R`, hay que estudiar la salida de la función `summary` aplicada a la función `lm`:

```{r,eval=FALSE}
summary(lm(y~x))
```

<div class="example">
La salida anterior para los datos de nuestro ejemplo es la siguiente:
```{r,eval=FALSE}
summary(lm(tensión ~ sal))
```

</div>


## Ejemplo
<div class="example">
```{r,echo=FALSE}
summary(lm(tensión ~ sal))
```

</div>

## Ejemplo
<div class="example">
En primer lugar `R` nos da los errores de los datos cometidos al estimar los valores $y_i$ por $\widehat{y}_i$.

A continuación, en una tabla, nos da las estimaciones de los parámetros $\beta_0$ y $\beta_1$, $b_0$ y $b_1$, en la columna `Estimate`, los errores estándar de dichos estimadores en la columna `Std. Error` y el valor del estadístico $t$ en la columna `t value`. En la última columna nos da el p-valor para el contraste anterior.

Observamos que hay dos valores de los estadísticos de contraste, uno corresponde al contraste para $\beta_1$ (la segunda fila) y otro corresponde al contraste para $\beta_0$ que no tiene ningún interés para ver si la regresión tiene sentido. Pensemos que el hecho de que el parámetro $\beta_0$ sea nulo no contradice el hecho de que la variable $X$ tenga efecto en la variable $Y$. 

Por último, en el último párrafo de la salida, nos da el valor del error residual o la estimación de $S$, el valor del coeficiente de determinación $R^2$, el `Multiple R-squared`, y el valor de $R^2$ ajustado del que hablaremos más adelante.
</div>

## Ejemplo
<div class="example">
En la última fila, nos habla del estadístico $F$ que es otra manera de realizar el contraste sobre el parámetro $\beta_1$ que hemos explicado anteriormente pero en lugar de usar el estadístico $T$, se usa el estadístico $T^2$ que, si la hipótesis nula es cierta, se distribuye según una $F$ de Fisher con 1 y $n-2=4$ grados de libertad. Podéis comprobar por ejemplo que $t_0^2$ vale el valor del `F-statistic`:
```{r}
t0^2
```
</div>

# Regresión lineal múltiple

## Introducción

En la **regresión lineal simple**, estudiábamos si una **variable dependiente** $Y$ dependía linealmente de una **variable independiente o de control** $X$.

En la práctica, dicha situación raramente se da ya que la **variable dependiente o de respuesta** $Y$ suele depender de más de una **variable de control**.

Por tanto, en esta sección vamos a generalizar todo el estudio que hemos hecho para la **regresión lineal simple** donde sólo hay una **variable de control** al caso en que tengamos $k$ variables de control $X_1,\ldots, X_k$.

En resumen, suponemos que tenemos una **variable dependiente** $Y$ y $k$ **variables independientes o de control** $X_1,\ldots, X_k$.


## Modelo de regresión lineal múltiple
De forma similar a la **regresión lineal simple**, suposemos que la relación de la media de la variable aleatoria $Y$, fijados $k$ valores de las variables  $X_1,\ldots, X_k$, $x_1,\ldots,x_k$, es la siguiente:
$$
\mu_{Y|x_1,\ldots,x_k}= \beta_0+\beta_1 x_1+\cdots+\beta_k x_k.
$$
Es decir, la media la media de la variable aleatoria $Y_{x_1,\ldots,x_k}$ es una función lineal de los valores $x_1,\ldots,x_k$.

## Modelo de regresión lineal múltiple
Los valores $\beta_0,\beta_1,\ldots,\beta_k$ son los llamados **parámetros de regresión** y se tienen que estimar a partir de una muestra de las variables consideradas:
$$
(x_{i1},x_{i2},\ldots,x_{ik},y_i)_{i=1,\ldots,n}
$$
Para que dichas estimaciones se puedan realizar, hay que suponer que $n>k$ ya que en caso contrario tendríamos un problema *subestimado*: tendríamos más parámetros que valores en la muestra.

Denotaremos el vector $\underline{x}_i$ al conjunto de los $k$ valores del individuo $i$-ésimo: $\underline{x}_i= (x_{i1},x_{i2},\ldots,x_{ik})$.


## Modelo de regresión lineal múltiple
Escribimos el modelo de **regresión lineal múltiple** de la forma siguiente:
$$
Y|x_1,\ldots,x_k=\beta_0+\beta_1 x_1+\cdots+\beta_{k} x_k+E_{x_1,\ldots,x_k},
$$
donde

* $Y|x_1,\ldots,x_k$ es la v.a. que da el valor de $Y$ cuando cada $X_i$ vale $x_i$, $X_i=x_i$,

* $E_{x_1,\ldots,x_k}$ son las v.a. error, o residuales, y representan el error aleatorio del modelo asociado a $(x_1,\ldots,x_k)$.



## Modelo de regresión lineal múltiple
A partir de una muestra
$$
(\underline{x}_{i},y_i)_{i=1,2,\ldots,n}
$$
vamos a obtener estimaciones $b_0,b_1,\ldots,b_k$ de los **parámetros de regresión** $\beta_0,\beta_1,\ldots,\beta_k$.

## Modelo de regresión lineal múltiple

Una vez obtenidas las estimaciones $b_0,b_1,\ldots,b_k$, podemos definir los valores siguientes:
$$
\begin{array}{l}
\widehat{y}_i=b_0+b_1 x_{i 1}+\cdots+b_{k} x_{i k},\\
y_i=b_0+b_1 x_{i 1}+\cdots+b_{k} x_{i k}+e_i,
\end{array}
$$
donde 

* $\widehat{y}_i$ es el valor predicho de a partir de $y_i$ y $\underline{x}_{i}$ y

* $e_i$ estima el error $E_{\underline{x}_{i}}$: $e_i=y_i-\widehat{y}_i$.


## Modelo de regresión lineal múltiple
Para simplificar la notación, escribimos los datos de la muestra en forma matricial. 

En primer lugar, definimos los vectores siguientes:
$$
\mathbf{y}=
\left(
\begin{array}{l}
y_1\\ y_2\\ \vdots\\ y_n
\end{array}
\right),\ \mathbf{b}=\left(
\begin{array}{l}
b_0\\ b_1 \\ \vdots\\b_k
\end{array}
\right),\ \mathbf{\widehat{y}}=\left(
\begin{array}{l}
\widehat{y}_1\\ \widehat{y}_2\\ \vdots\\\widehat{y}_n
\end{array}
\right),\ \mathbf{e}=\left(
\begin{array}{l}
e_1\\ e_2\\ \vdots\\ e_n
\end{array}
\right).
$$


## Modelo de regresión lineal múltiple
Definimos la matriz $\mathbf{X}$ a partir de los datos de la muestra de las variables $X_i$, $i=1,\ldots,k$:
$$
\mathbf{X}=\left(
\begin{array}{lllll}
1&x_{11}&x_{12}&\ldots&x_{1k}\\
1&x_{21}&x_{22}&\ldots&x_{2k}\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
1&x_{n1}&x_{n2}&\ldots&x_{nk}
\end{array}
\right).
$$

## Modelo de regresión lineal múltiple
Las ecuaciones
$$
\begin{array}{l}
\widehat{y}_i=b_0+b_1 x_{i 1}+\cdots+b_{k} x_{i k},\\
y_i=b_0+b_1 x_{i 1}+\cdots+b_{k} x_{i k}+e_i,
\end{array}
$$
se escriben en forma matricial de la forma siguiente:
$$
\begin{array}{l}
\mathbf{\widehat{y}} = \mathbf{X}\cdot\mathbf{b}, \\
\mathbf{y}=   \mathbf{X}\cdot \mathbf{b}+\mathbf{e}.\\
\end{array}
$$

## Método de los mínimos cuadrados

Definimos el **error cuadrático** $SS_E$ cómo:
$$
SS_E=\sum\limits_{i=1}^n
e^2_i=\sum\limits_{i=1}^n (y_i-\widehat{y}_i)^2 
=\sum\limits_{i=1}^n (y_i-b_0-b_1 x_{i 1}-\cdots -b_{k} x_{ik})^2.
$$

Los **estimadores** de los **parámetros** $\beta_0,\beta_1,\ldots, \beta_k$ por el método de **mínimos cuadrados** serán los
valores $b_0,b_1,\ldots, b_k$ que minimicen $SS_E$.

Para calcularlos, calculamos las derivadas parciales del error cuadrático $SS_E$ respecto cada $b_i$, las igualamos a 0, las resolvemos, y comprobamos que la solución $(b_0,\ldots,b_k)$ encontrada corresponde a un mínimo.

## Método de los mínimos cuadrados
El teorema siguiente nos da la expresión de dichos **estimadores**:

<l class="prop">Teorema.</l>
Los **estimadores** por el método de los **mínimos cuadrados** de los parámetros $\beta_0,\beta_1,\ldots,\beta_k$ a partir de la muestra 
$(\underline{x}_{i},y_i)_{i=1,2,\ldots,n}$ son los siguientes:
$$
\mathbf{b}=\left(\mathbf{X}^t\cdot \mathbf{X}
\right)^{-1}\cdot \left(\mathbf{X}^t \cdot \mathbf{y}\right).
$$

## Ejemplo
<div class="example">
**Ejemplo**

Se postula que la altura de un bebé ($y$) tiene
una relación  lineal con su edad en días ($x_1$), su altura al nacer en cm. ($x_2$), su peso en kg. al nacer ($x_3$) y el aumento en tanto por ciento de su peso actual respecto de su peso al nacer ($x_4$)

El modelo es:
$$
\mu_{Y|x_1,x_2,x_3,x_4}=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3+\beta_4x_4.
$$
En una muestra de $n=9$ niños, se obtuvieron los resultados siguientes:

<div class="center">
|$y$ | $x_1$ | $x_2$ | $x_3$ | $x_4$|
|:--:|:--:|:--:|:--:|:--:|
|57.5|78|48.2|2.75|29.5| 
|52.8|69|45.5|2.15|26.3|
|61.3|77|46.3|4.41|32.2| 
</div>


</div>

## Ejemplo
<div class="example">
<div class="center">
|$y$ | $x_1$ | $x_2$ | $x_3$ | $x_4$|
|:--:|:--:|:--:|:--:|:--:|
|67|88|49|5.52|36.5| 
|53.5|67|43|3.21|27.2|
|62.7|80|48|4.32|27.7| 
|56.2|74|48|2.31|28.3| 
|68.5|94|53|4.3|30.3|
|69.2|102|58|3.71|28.7|
</div>

</div>

## Ejemplo
<div class="example">
Hallemos los estimadores de los parámetros $\beta_0,\beta_1,\beta_2,\beta_3$ y $\beta_4$.

En primer lugar, hallamos la matriz $\mathbf{X}$ y el vector $\mathbf{y}$:
$$
\mathbf{X}=\left(
\begin{array}{ccccc}
1&78&48.2&2.75&29.5\\
1&69&45.5&2.15&26.3\\
1&77&46.3&4.41&32.2\\
1&88&49&5.52&36.5\\
1&67&43&3.21&27.2\\
1&80&48&4.32&27.7\\
1&74&48&2.31&28.3\\
1&94&53&4.3&30.3\\
1&102&58&3.71&28.7
\end{array}
\right),\
\mathbf{y}=\left(
\begin{array}{c}
57.5\\ 52.8\\ 61.3\\ 67\\ 53.5\\ 62.7\\ 56.2\\ 68.5\\ 69.2
\end{array}
\right).
$$
</div>


## Ejemplo
<div class="example">
El vector de los valores estimados $\mathbf{b}$ valdrá:
$$
\mathbf{b}=\left(\mathbf{X}^t\cdot \mathbf{X}
\right)^{-1}\cdot \left(\mathbf{X}^t \cdot \mathbf{y}\right).
$$
Realicemos los cálculos anteriores en `R`:
```{r,eval=FALSE}
X=matrix(c(1,78,48.2,2.75,29.5,1,69,45.5,2.15,26.3,
 1,77,46.3,4.41,32.2,1,88,49,5.52,36.5,
 1,67,43,3.21,27.2,1,80,48,4.32,27.7,
 1,74,48,2.31,28.3,1,94,53,4.3,30.3,
 1,102,58,3.71,28.7),nrow=9,byrow=TRUE)
y=cbind(c(57.5,52.8,61.3,67,53.5,62.7,56.2,68.5,69.2))
(estimaciones.b = solve(t(X)%*%X)%*%(t(X)%*%y))
```


</div>


## Ejemplo
<div class="example">
```{r,echo=FALSE}
X=matrix(c(1,78,48.2,2.75,29.5,1,69,45.5,2.15,26.3,
 1,77,46.3,4.41,32.2,1,88,49,5.52,36.5,
 1,67,43,3.21,27.2,1,80,48,4.32,27.7,
 1,74,48,2.31,28.3,1,94,53,4.3,30.3,
 1,102,58,3.71,28.7),nrow=9,byrow=TRUE)
y=cbind(c(57.5,52.8,61.3,67,53.5,62.7,56.2,68.5,69.2))
(estimaciones.b = solve(t(X)%*%X)%*%(t(X)%*%y))
```
La función lineal de regresión buscada se:
$$
\widehat{y}=`r estimaciones.b[1]`+ `r estimaciones.b[2]` x_1+`r estimaciones.b[3]` x_2 +`r estimaciones.b[4]` x_3 `r estimaciones.b[5]` x_4.
$$
Podemos observar, por ejemplo, que cuánto más edad tenga el niño, más altura tiene, cuánto más peso al nacer, más altura tiene pero cuánto más haya aumentado su peso respecto de su peso al nacer, su altura disminuye aunque dicha disminución es minúscula.

</div>

## Cálculo de la función de regresión en `R`
Para calcular la función de regresión en `R` hay que usar la función `lm`:
```{r,eval = FALSE}
lm(y ~ x1+x2+...+xk)
```

<div class="example">
En nuestro ejemplo, se calcula de la forma siguiente:
```{r}
lm(y ~ X[,2]+X[,3]+X[,4]+X[,5])
```

</div>

## Propiedades de la función de regresión
* La **función de regresión** pasa por el vector medio $(\overline{x}_1,\overline{x}_2,\ldots,\overline{x}_k,\overline{y})$:
$$
\overline{y}=b_0+b_1 \overline{x}_1+\cdots+b_k \overline{x}_k.
$$

* La media de los valores estimados se igual a la media de los observados:
$$
\overline{\widehat{y}}=\overline{y}.
$$

* Los errores $(e_i)_{i=1,\ldots,n}$  tienen media 0 y varianza:
$$
\tilde{s}_e^2=\frac{SS_E}{n-1}.
$$

## Ejemplo
<div class="example">
Verifiquemos las propiedades anteriores para los datos de nuestro ejemplo:

* La **función de regresión** pasa por el vector medio $(\overline{x}_1,\overline{x}_2,\ldots,\overline{x}_k,\overline{y})$:
```{r}
vectores.medios = apply(X[,1:5],2,mean)
round(mean(y)-t(estimaciones.b)%*%vectores.medios,6)
```
* La media de los valores estimados se igual a la media de los observados:
```{r}
valores.estimados = X%*%estimaciones.b
round(mean(y)-mean(valores.estimados),6)
```

</div>


## Ejemplo
<div class="example">
* Los errores $(e_i)_{i=1,\ldots,n}$  tienen media 0 y varianza $\tilde{s}_e^2=\frac{SS_E}{n-1}.$
```{r}
errores=y-valores.estimados
round(mean(errores))
SSE=sum(errores^2)
n=dim(X)[1]
var(errores)-SSE/(n-1)
```


</div>

## Coeficiente de determinación
Al igual que hicimos que la **regresión lineal simple**, vamos a definir el **coeficiente de determinación** que es una manera (no la única como ya hemos comentado) de medir lo efectiva que es la regresión.

Introducimos las **variabilidades** siguientes tal como hicimos en la **regresión lineal simple**:

## Coeficiente de determinación
* **Variabilidad total** o suma total de cuadrados: $SS_T =\sum\limits_{i=1}^n(y_i-\overline{y})^2 = (n-1)\cdot \tilde{s}_y^2.$

* **Variabilidad de la regresión** o suma de cuadrados de la regresión: $SS_R=\sum\limits_{i=1}^n(\widehat{y}_i-\overline{y})^2=(n-1)\cdot \tilde{s}_{\widehat{y}}^2.$

* **Variabilidad del error** o suma de cuadrados del error: $SS_E=\sum\limits_{i=1}^n(y_i-\widehat{y}_i)^2=(n-1)\cdot \tilde{s}_e^2.$

Recordemos que tal como pasaba en la **regresión lineal simple**, la **variabilidad total** se puede descomponer como la suma de la **variabilidad de la regresión** y la **variabilidad del error**.

## Coeficiente de determinación
<l class="prop">Teorema. </l>
En una regresión lineal múltiple usando el método de los mínimos cuadrados, se cumple la siguiente relación entre las **variabilidades**:
$$
SS_T=SS_R+SS_E,
$$
o equivalentemente,
$$
\tilde{s}^2_y=\tilde{s}^2_{\widehat{y}}+\tilde{s}^2_e.
$$
En este caso, tal como comentábamos en la **regresión lineal simple**, cuántas más "proximas" estén las **variabilidades** $SS_T$ y $SS_R$, o, si se quiere, $\tilde{s}^2_y$ y $\tilde{s}^2_{\widehat{y}}$, más efectiva habrá sida la regresión, ya que la regresión habrá heredado mucha variabilidad de los datos $y_i$, $i=1,\ldots,n$ y la variabilidad del error, $SS_E$ será pequeña.

## Coeficiente de determinación
Definimos el **coeficiente de determinación** en una **regresión lineal múltiple** como:
$R^2=\frac{SS_R}{SS_T}=\frac{\tilde{s}^2_{\widehat{y}}}{\tilde{s}^2_y},$
y representa la fracción de la variabilidad de $y$ que queda explicada
por la variabilidad del modelo de regresión lineal.

De la misma manera, definimos el **coeficiente de correlación múltiple** de  $y$ respecto de  $x_1,\ldots, x_k$ como $R=\sqrt{R^2}$.


## Propiedades del coeficiente de determinación
El **coeficiente de determinación** verifica las dos primeras propiedades que verificaba el **coeficiente de determinación** en el caso de la **regresión lineal simple**:

* El **coeficiente de determinación** es una cantidad entre 0 y 1: $0\leq R^2\leq 1$. Entonces, cuánto más próximo a 1 esté dicho coeficiente, más precisa será la recta de regresión.

* El **coeficiente de determinación** se puede expresar en función de la **variabilidad del error** de la forma siguiente:
$$
R^2=\frac{SS_T-SS_E}{SS_T}=1-\frac{SS_E}{SS_T}=1-\frac{\tilde{s}_e^2}{\tilde{s}_y^2}.
$$


## Ejemplo
<div class="example">
Vamos a calcular las variabilidades y el coeficiente de determinación para los datos del ejemplo que estamos trabajando:

* **Variabilidad total**:
```{r}
(SST=sum((y-mean(y))^2))
```
* **Variabilidad de la regresión**:
```{r}
(SSR=sum((valores.estimados-mean(y))^2))
```
</div>

## Ejemplo
<div class="example">

* **Variabilidad del error**
```{r}
(SSE = sum(errores^2))
```
Comprobemos que la **variabilidad total** se descompone en la suma de la **variabilidad de la regresión** y la **variabilidad del error**:
```{r}
round(SST-SSR-SSE,6)
```



</div>

## Ejemplo
<div class="example">
El coeficiente de determinación será:
```{r}
(R2=SSR/SST)
```
o, si se quiere,
```{r}
(R2 = var(valores.estimados)/var(y))
```



</div>

## Coeficiente de determinación en `R`
Para hallar el **coeficiente de determinación** en `R` podemos usar las mismas funciones que usábamos en la **regresión lineal simple**:
```{r,eval=FALSE}
summary(lm(y~x1+...+xk))$r.squared
```

<div class="example">
Para los datos de nuestro ejemplo, hemos de hacer lo siguiente:
```{r}
summary(lm(y~X[,2]+X[,3]+X[,4]+X[,5]))$r.squared
```


</div>

## Coeficiente de determinación ajustado

El **coeficiente de determinación** definido anteriormente aumenta si aumentamos el número de variables independientes $k$, incluso si éstas aportan información redundante o poca información. Por ejemplo, si añadimos variables que son linealmente dependientes de las demás.

Para evitar este problema o para penalizar el aumento de variables independientes se usa en su lugar el **coeficiente de regresión ajustado**: 
$$
R^2_{adj}=\frac{MS_T-MS_E}{MS_T},
$$
donde $MS_T=\frac{SS_T}{n-1},\quad MS_E=\frac{SS_E}{n-k-1}.$

## Coeficiente de determinación ajustado

Fijarse ahora que si aumentamos $k$, el valor de $MS_E$ aumenta y el valor de $R^2_{adj}$ disminuirá. Por tanto, con el $R^2_{adj}$ penalizamos el aumento de variables independientes.

Si aumentamos el nombre de variables independientes con variables explicativas que aporten mucha información a la regresión, el valor de $SS_E$ disminuirá haciendo que se mantenga estable el valor de $MS_E$ y el valor de $R^2_{adj}$ no disminuirá.

La relación entre el **coeficiente de determinación ajustado** y el **coeficiente de determinación** es la siguiente:
$$
R^2_{adj}=1-(1-R^2)\frac{n-1}{n-k-1}.
$$

## Ejemplo
<div class="example">
Calculemos el **coeficiente de determinación ajustado** para los datos de nuestro ejemplo:
```{r}
k=dim(X)[2]-1
(R2.adj = 1-(1-R2)*(n-1)/(n-k-1))
```
Observamos que obtenemos un **coeficiente de determinación ajustado** menor que el **coeficiente de determinación**. 
</div>

En general, lo que hemos observado en el ejemplo anterior, ocurre siempre, es decir $0\leq R^2_{adj} < R^2\leq 1$, lo que nos permite concluir que para el **coeficiente de determinación ajustado**, es más difícil obtener un valor cercano a 1 que para el **coeficiente de determinación**.

## Coeficiente de determinación ajustado en `R`
El cálculo del **coeficiente de determinación ajustado** en `R` es parecido al cálculo del **coeficiente de determinación**: sólo hemos de cambiar el parámetro `r.squared` por `adj.r.squared`
```{r,eval=FALSE}
summary(lm(y~x1+...+xk))$adj.r.squared
```

<div class="example">
Para los datos de nuestro ejemplo, hemos de hacer lo siguiente:
```{r}
summary(lm(y~X[,2]+X[,3]+X[,4]+X[,5]))$adj.r.squared
```

## Comparación de modelos
Imaginemos que tenemos el modelo 
$$
Y_{|x_1,\ldots,x_k}=\beta_0 + \beta_1 x_1+\cdots + \beta_k x_k + E_{x_1,\ldots,x_k},
$$
y añadimos una nueva variable independiente $x_{k+1}$. Entonces, tendremos otro modelo:
$$
Y_{|x_1,\ldots,x_k,x_{k+1}}=\beta_0 + \beta_1 x_1+\cdots + \beta_k x_k +\beta_{k+1}+ E_{x_1,\ldots,x_k,x_{k+1}}.
$$
¿Cómo podemos comparar los dos modelos anteriores?

O, dicho en otras palabras, ¿cómo decidir si la nueva variable introducida $x_{k+1}$ aporta información relevante?

## Comparación de modelos
Una manera de realizar dicha comparación es usando el **coeficiente de regresión ajustado** $R^2_{adj}$: el modelo que tenga mayor $R^2_{adj}$ es el mejor. 

Por tanto, si el valor del **coeficiente de regresión ajustado** del segundo modelo supera el **coeficiente de regresión ajustado** del primero, diremos que la nueva variable $x_{k+1}$ aporta información relevante y hay que tenerla en cuenta.

## Ejemplo
<div class="example">
Con los datos de nuestro ejemplo, consideremos sólo la variable independiente correspondiente a la segunda columna de la matriz $\mathbf{X}$, variable que corresponde a la edad del niño en días. 

El **coeficiente de correlación ajustado** de este modelo que, por cierto, sería de **regresión lineal simple** vale:
```{r}
summary(lm(y~X[,2]))$adj.r.squared
```

Consideremos ahora las dos primeras variables correspondientes a las columnas segunda y tercera de la matriz $\mathbf{X}$, variables que corresponden a la edad del niño en días y su altura en nacer. 

El **coeficiente de correlación ajustado** de este modelo vale:
```{r}
summary(lm(y~X[,2]+X[,3]))$adj.r.squared
```
</div>

## Ejemplo
<div class="example">
Hemos obtenido un **coeficiente de regresión ajustado** superior. Por tanto, ha valido la pena añadir la variable correspondiente a la altura al nacer del niño ya que nos ha aportado información relevante al modelo.

Consideremos ahora las tres primeras variables correspondientes a las columnas segunda, tercera y cuarta de la matriz $\mathbf{X}$, variables que corresponden a la edad del niño en días, su altura en nacer y su peso al nacer.

El **coeficiente de correlación ajustado** de este modelo vale:
```{r}
summary(lm(y~X[,2]+X[,3]+X[,4]))$adj.r.squared
```
Hemos vuelto a obtener un **coeficiente de regresión ajustado** superior al anterior. Por tanto, también ha valido la pena añadir la variable peso del niño al nacer al aportar dicha variable información relevante al modelo.

Recordemos que el **coeficiente de regresión ajustado** del modelo completo era `r summary(lm(y~X[,2]+X[,3]+X[,4]+X[,5]))$adj.r.squared`, valor inferior al obtenido anteriormente.

Por tanto, la última variable, el aumento del peso actual del niño respecto de su peso al nacer, no aporta información relevante y no debe ser considerada en el modelo de regresión lineal múltiple.
</div>

## Comparación de modelos

Comparar dos modelos usando el **coeficiente de regresión ajustado** es uno de los métodos que hay en la literatura para ver si un método es más adecuado que otro.

Existen más métodos como son el **AIC** (*Akaike's Information Criterion*) o el método **BIC** (*Bayesian Information Criterion*).

El método **AIC** cuantifica cuánta información de $Y$ se pierde con el modelo y cuántas variables usamos: el mejor modelo es el que tiene un valor de **AIC** más pequeño. Concretamente, se calcula la cantidad siguiente: $AIC=n\ln(SS_E/n)+2k$ y el modelo con menor **AIC** es el más adecuado.

Para usar el método **AIC** en `R`, hay que usar la función `AIC`:
```{r,eval=FALSE}
AIC(lm(y~x1+...+xk))
```


## Ejemplo
<div class="example">
Veamos usando el método **AIC** cuál de los cuatro modelos vistos anteriormente para los datos de nuestro ejemplo es el más adecuado:
```{r}
AIC(lm(y~X[,2]))
AIC(lm(y~X[,2]+X[,3]))
AIC(lm(y~X[,2]+X[,3]+X[,4]))
```
</div>


## Ejemplo
<div class="example">
```{r}
AIC(lm(y~X[,2]+X[,3]+X[,4]+X[,5]))
```
El método **AIC** concuerda con el método del **coeficiente de determinación ajustado**. El mejor modelo es el que incluye las variables correspondientes a las columnas 2, 3 y 4 de la matriz $\mathbf{X}$ que, recordemos, son las variables la edad del niño en días, su altura en nacer y su peso al nacer.
</div>

## Comparación de modelos

El método **BIC** cuantifica cuánta información de $Y$ se pierde con el modelo y cuántas variables y datos usamos: el mejor modelo es el que tiene un valor de **BIC** más pequeño. Para saber si un modelo es mejor que otro, hay que calcular la cantidad siguiente: 
$BIC=n\ln(SS_E/n)+k\ln(n)$ y, como ya hemos comentado, el modelo con menor **BIC** es el más adecuado.

Para usar el método **BIC** en `R`, hay que usar la función `BIC`:
```{r,eval=FALSE}
BIC(lm(y~x1+...+xk))
```

## Ejemplo
<div class="example">
Veamos usando el método **BIC** cuál de los cuatro modelos vistos anteriormente para los datos de nuestro ejemplo es el más adecuado:
```{r}
BIC(lm(y~X[,2]))
BIC(lm(y~X[,2]+X[,3]))
BIC(lm(y~X[,2]+X[,3]+X[,4]))
```
</div>


## Ejemplo
<div class="example">
```{r}
BIC(lm(y~X[,2]+X[,3]+X[,4]+X[,5]))
```
El método **BIC** concuerda con los dos métodos usados anteriormente: el método del **coeficiente de determinación ajustado** y el método **AIC**. El mejor modelo es el que incluye las variables correspondientes a las columnas 2, 3 y 4 de la matriz $\mathbf{X}$ que, recordemos, son las variables la edad del niño en días, su altura en nacer y su peso al nacer.
</div>


## Intervalos de confianza
Vamos a calcular los intervalos de confianza para los $k+1$ parámetros de modelo $\beta_0,\beta_1,\ldots,\beta_k$.

Para ello, al igual que hicimos en la **regresión lineal simple**, supondremos que las variables aleatorias error $E_i=E_{\underline{x}_{i}}$ son incorreladas, es decir, la covarianza entre un par de ellas cualesquiera es cero y todas normales de media 0 y de misma varianza $\sigma_E^2$.

Antes de dar los intervalos de confianza, necesitamos conocer las propiedades sobre los estimadores de los parámetros anteriores, es decir, si son insesgados, cómo estimar la varianza común $\sigma_E^2$ y sus errores estándar.

## Intervalos de confianza
Dichas propiedades vienen dadas en los teoremas siguientes:

<l class="prop">Teorema. </l>
Bajo las hipótesis anteriores, los estimadores $b_0,\ldots, b_k$ de los parámetros
$\beta_0,\ldots,\beta_k$ son máximo verosímiles y además no sesgados.

<l class="prop">Teorema. </l>
Bajo las hipótesis anteriores, 
$$
\mathrm{Cov}(b_0,b_1,\ldots,b_k)= \sigma_E^2\cdot (\mathbf{X}^\top\cdot \mathbf{X})^{-1},
$$
donde $\mathrm{Cov}(b_0,b_1,\ldots,b_k)$ es la matriz de covarianzas de los estimadores $b_0,b_1,\ldots b_k$ de los parámetros $\beta_0,\beta_1,\ldots,\beta_k$ de componentes $\mathrm{Cov}(b_0,b_1,\ldots,b_k)_{ij}=\mathrm{Cov}(b_i,b_j)$, $i,j=0,1\ldots,k$ y un estimador no sesgado de la varianza común $\sigma_E^2$ es $S^2=\frac{SS_E}{n-k-1}$.


## Intervalos de confianza

<l class="prop">Teorema. </l>
Bajo las hipótesis anteriores, el error estándar de cada estimador $b_i$ vale
$\sqrt{(\sigma_E^2\cdot (\mathbf{X}^\top \mathbf{X})^{-1})_{ii}},$
(la raíz cuadrada de la $i$-èsima entrada de la diagonal de la matriz $\sigma_E^2\cdot (\mathbf{X}^\top \mathbf{X})^{-1}$ empezando por $i=0$.)

<l class="prop">Teorema. </l>
Bajo las hipótesis anteriores, 

* la variable aleatoria  $\frac{\beta_i-b_i}{\sqrt{(S^2\cdot (\mathbf{X}^\top \mathbf{X})^{-1})_{ii}}},$
sigue un ley $t$ de Student cono $n-k-1$ grados de libertad,

* un intervalo de confianza del $(1-\alpha)\cdot 100\%$ de confianza para el parámetro $\beta_i$ es
$b_i\pm t_{n-k-1,1-\frac{\alpha}2}\cdot \sqrt{(S^2\cdot (\mathbf{X}^\top \mathbf{X})^{-1})_{ii}}.$


## Ejemplo
<div class="example">
Veamos si los errores de los datos del ejemplo que vamos desarrollando se distribuyen normalmente usando el test de **Kolmogorov-Smirnov-Lilliefors**:

```{r}
lillie.test(errores)
```
Como el p-valor obtenido es muy grande, concluimos que no tenemos evidencias suficientes para rechazar la normalidad de los errores.
</div>


## Ejemplo
<div class="example">
La estimación de la varianza común $S^2$ será:
```{r}
(S2 = SSE/(n-k-1))
```

La estimación de la matriz de covarianzas de los estimadores $b_0,b_1,b_2,b_3,b_4$ es la siguiente:
```{r}
S2*solve(t(X)%*%X)
```


</div>

## Ejemplo
<div class="example">
En la matriz anterior, podemos observar que el estimador con más varianza sería $b_0$ seguido de $b_3$ que corresponde a la varianza del peso del niño al nacer.

También podemos observar que entre las variables $x_2$ (altura del niño al nacer) y $x_3$ (peso del niño al nacer) existe una gran correlación lineal: `r (S2*solve(t(X)%*%X))[4,3]`.

Las estimaciones de los errores estándar de los estimadores $b_0,b_1,b_2,b_3,b_4$ son las siguientes:
```{r}
(errores.estandar = sqrt(S2*diag(solve(t(X)%*%X))))
```
</div>

## Ejemplo
<div class="example">
Un intervalo de confianza para los estimadores  $b_0,b_1,b_2,b_3,b_4$ al 95% de confianza es el siguiente:

* Intervalo de confianza para $b_0$ al 95% de confianza:
```{r}
alpha=0.05
c(estimaciones.b[1]-qt(1-alpha/2,n-k-1)*errores.estandar[1],
  estimaciones.b[1]+qt(1-alpha/2,n-k-1)*errores.estandar[1])
```


* Intervalo de confianza para $b_1$ al 95% de confianza:
```{r}
c(estimaciones.b[2]-qt(1-alpha/2,n-k-1)*errores.estandar[2],
  estimaciones.b[2]+qt(1-alpha/2,n-k-1)*errores.estandar[2])
```
</div>

## Ejemplo
<div class="example">

* Intervalo de confianza para $b_2$ al 95% de confianza:
```{r}
c(estimaciones.b[3]-qt(1-alpha/2,n-k-1)*errores.estandar[3],
  estimaciones.b[3]+qt(1-alpha/2,n-k-1)*errores.estandar[3])
```


* Intervalo de confianza para $b_3$ al 95% de confianza:
```{r}
c(estimaciones.b[4]-qt(1-alpha/2,n-k-1)*errores.estandar[4],
  estimaciones.b[4]+qt(1-alpha/2,n-k-1)*errores.estandar[4])
```
</div>

## Ejemplo
<div class="example">

* Intervalo de confianza para $b_4$ al 95% de confianza:
```{r}
c(estimaciones.b[5]-qt(1-alpha/2,n-k-1)*errores.estandar[5],
  estimaciones.b[5]+qt(1-alpha/2,n-k-1)*errores.estandar[5])
```

</div>

## Intervalos de confianza en `R`
Recordemos que para hallar los intervalos de confianza en `R` había que usar la función `confint` aplicado al objeto `lm(...)`. El parámetro `level` nos da el nivel de confianza con valor por defecto 0.95:

```{r,eval=FALSE}
confint(lm(y~x1+...+xk))
```

## Ejemplo
<div class="example">
Si aplicamos la función anterior a los datos de nuestro ejemplo, obtenemos los intervalos de confianza para los estimadores $b_0,b_1,b_2,b_3,b_4$ que ya hemos obtenido antes:

```{r}
confint(lm(y~X[,2]+X[,3]+X[,4]+X[,5]),level=0.95)
```
</div>

## Intervalos de confianza
Tal como hemos hecho en la **regresión lineal simple**, fijados unos valores concretos de las variables independientes $(x_{10},\ldots,x_{k0})$ podemos considerar dos parámetros más a estudiar: el valor medio de la variable aleatoria $Y|x_{10},\ldots,x_{k0}$, $\mu_{Y|x_{10},\ldots,x_{k0}}$ y el valor estimado $y_0=b_0+b_1\cdot x_{10}+\cdots + b_k\cdot x_{k0}$ por la función de regresión.

Recordemos que los dos intervalos de confianza anteriores nos ayudan a interpretar la regresión cuando el valor de las variables $(X_1,\ldots,X_k)$ valen un determinado valor $(x_{10},\ldots,x_{k0})$.

Para realizar la estimación puntual de los dos parámetros anteriores usaremos el mismo estimador $\hat{y}_0 =b_0+b_1 x_1+\cdots + b_k x_k$ pero tal como pasaba en la **regresión lineal simple**, los errores estándard no serán los mismos tal como nos dicen el teorema siguiente.

## Intervalos de confianza

<l class="prop">Teorema. </l>
Sean $\underline{x}_0=(x_{01},\ldots,x_{0k})$ unos valores concretos de las variables $(X_1,\ldots, X_k)$.

Bajo las hipótesis anteriores, 

* el error estándar de $\widehat{y}_0$ como estimador del parámetro $\mu_{Y|\underline{x}_0}$ es el siguiente:
$$
S\sqrt{\mathbf{x}_0\cdot  (\mathbf{X}^\top \cdot\mathbf{X})^{-1}\cdot \mathbf{x}_0^\top},
$$

* el error estándar de $\widehat{y}_0$ como estimador de $y_0$ es el siguiente:
$$
S\sqrt{1+\mathbf{x}_0\cdot  (\mathbf{X}^\top \cdot\mathbf{X})^{-1}\cdot \mathbf{x}_0^\top},
$$
donde $\mathbf{x}_0 =(1,\underline{x}_0)=(1,x_{01},\ldots,x_{0k})$.


## Intervalos de confianza
Los estimadores para hallar los intervalos de confianza para los parámetros $\mu_{Y|x_{10},\ldots,x_{k0}}$ y el valor estimado $y_0$ vienen dados por el teorema siguiente:

<l class="prop">Teorema. </l>
Sean $\underline{x}_0=(x_{01},\ldots,x_{0k})$ unos valores concretos de las variables $(X_1,\ldots, X_k)$.
Bajo nuestras hipótesis, las variables aleatorias $\dfrac{\mu_{Y|\underline{x}_0}-\widehat{y}_0}{S\sqrt{\mathbf{x}_0\cdot (\mathbf{X}^\top \cdot \mathbf{X})^{-1}\cdot \mathbf{x}_0^\top}},\ 
\dfrac{y_0-\widehat{y}_0}{S\sqrt{1+\mathbf{x}_0\cdot  (\mathbf{X}^\top \cdot \mathbf{X})^{-1}\cdot \mathbf{x}_0^\top}},$
siguen la ley $t$ de Student con $n-k-1$ grados de libertad.

## Intervalos de confianza
Por último, bajo las hipótesis anteriores, un intervalo de confianza al $100\cdot (1-\alpha)$% de confianza para cada uno de los parámetros $\mu_{Y|x_{10},\ldots,x_{k0}}$ y el valor estimado $y_0$ es el siguiente:

* Parámetro $\mu_{Y|x_{10},\ldots,x_{k0}}$: $\hat{y}_0\pm t_{n-k-1,1-\frac{\alpha}{2}}\cdot S\sqrt{\mathbf{x}_0\cdot (\mathbf{X}^\top \cdot \mathbf{X})^{-1}\cdot \mathbf{x}_0^\top}$.

* Parámetro $y_0$: $\hat{y}_0\pm t_{n-k-1,1-\frac{\alpha}{2}}\cdot S\sqrt{1+\mathbf{x}_0\cdot (\mathbf{X}^\top \cdot \mathbf{X})^{-1}\cdot \mathbf{x}_0^\top}$.


## Ejemplo
<div class="example">
Hallemos un intervalo de confianza para los datos de nuestro ejemplo suponiendo un niño con $x_{10}=75$ días de edad, de altura $x_{20}=50$ cm. al nacer, de $x_{30}=4$ kg. al nacer y con un $x_{40}=30$% de aumento de peso con respecto su peso al nacer.

El intervalo de confianza para el parámetro $\mu_{Y|x_{10}=75,x_{20}=50,x_{30}=4,x_{40}=30}$ al 95% de confianza es el siguiente:
```{r}
alpha=0.05
x0 = c(1,75,50,4,30)
y0.estimado = sum(estimaciones.b*x0)
c(y0.estimado-qt(1-alpha/2,n-k-1)*sqrt(S2*(t(x0)%*%solve(t(X)%*%X)%*%x0)),
  y0.estimado+qt(1-alpha/2,n-k-1)*sqrt(S2*(t(x0)%*%solve(t(X)%*%X)%*%x0)))
```
</div>

## Ejemplo
<div class="example">
El intervalo de confianza para el parámetro $y_0$ al 95% de confianza es el siguiente:
```{r}
c(y0.estimado-qt(1-alpha/2,n-k-1)*sqrt(S2*(1+(t(x0)%*%solve(t(X)%*%X)%*%x0))),
  y0.estimado+qt(1-alpha/2,n-k-1)*sqrt(S2*(1+(t(x0)%*%solve(t(X)%*%X)%*%x0))))
```

</div>

## Intervalos de confianza en `R`
Para hallar el intervalo de confianza para el parámetros $\mu_{Y|x_{10},\ldots,x_{k0}}$ al $100\cdot (1-\alpha)$% de confianza hay que usar la función `predict.lm` de la forma siguiente:
```{r,eval=FALSE}
newdata=data.frame(x1=x10,...,xk=xk0)
predict.lm(lm(y~x1+...+xk),newdata,interval="confidence",level= nivel.confianza)
```

Para el parámetro $y_0$ hay que usar la misma función anterior pero cambiando el parámetro `interval` al valor `prediction`:

```{r,eval=FALSE}
newdata=data.frame(x1=x10,...,xk=xk0)
predict.lm(lm(y~x1+...+xk),newdata,interval="prediction",level= nivel.confianza)
```


## Ejemplo
<div class="example">
Para los datos del ejemplo anterior, los intervalos de confianza para los parámetros $\mu_{Y|x_{10}=75,x_{20}=50,x_{30}=4,x_{40}=30}$ y $y_0$ al 95% de confianza se calcularían en `R` de la forma siguiente:
```{r}
newdata=data.frame(x1=75,x2=50,x3=4,x4=30)
x1=X[,2]
x2=X[,3]
x3=X[,4]
x4=X[,5]
predict.lm(lm(y~x1+x2+x3+x4),newdata,interval="confidence",level= 0.95)
```
</div>

## Ejemplo
<div class="example">
```{r}
predict.lm(lm(y~x1+x2+x3+x4),newdata,interval="prediction",level= 0.95)
```


</div>

## Contrastes de hipótesis sobre los parámetros $\beta_i$

En el caso de la **regresión lineal múltiple** podemos plantearnos el contraste de hipótesis siguiente:
$$
\left.\begin{array}{l} 
H_0: \beta_1=\beta_2=\cdots=\beta_k=0, \\
H_1: \mbox{existe algún $i$ tal que }\beta_i \neq 0.
\end{array}
\right\}
$$
Es decir, queremos contrastar si la **regresión lineal múltiple** realizada ha tenido sentido ya que la hipótesis nula equivaldría a decir que ninguna variable independiente $X_i$, $i=1,\ldots,k$ ha tenido efecto sobre la variable $Y$ y como consecuencia, la regresión no ha tenido sentido.


## Contrastes de hipótesis sobre los parámetros $\beta_i$
Para realizar el contraste de hipótesis anterior, podríamos plantear los $k$ contrastes siguientes:
$$
\left.\begin{array}{l} H_0: \beta_i=0, \\
H_1: \beta_i \neq 0, \end{array}
\right\}
$$
usando como **estadístico de contraste** $\frac{\beta_i-b_i}{\sqrt{(S^2\cdot (\mathbf{X}^\top \mathbf{X})^{-1})_{ii}}}$,
que sabemos que sigue una ley $t$ de Student con $n-k-1$ grados de libertad.


## Contrastes de hipótesis sobre los parámetros $\beta_i$

El problema es que los $k$ contrastes anteriores no son **independientes**, es decir, los **estadísticos de contraste** son variables aleatorias **no independientes** entre otras cosas porque su matriz de covarianzas no tiene por qué ser diagonal. 

Por tanto, si realizamos el contraste de hipótesis original a partir de los $k$ contrastes anteriores, es muy complicado hallar el **error tipo I** $\alpha$ del contraste original a partir de los **errores tipo I** de cada uno de los $k$ contrastes al fallar la independencia de los mismos.

## Contrastes de hipótesis sobre los parámetros $\beta_i$

Otra posibilidad para evitar el problema comentado anteriormente es plantear el contraste original como un contraste **ANOVA** donde las subpoblaciones consideradas serían las variables $Y|\underline{x}_1, \ldots, Y|\underline{x}_n$ siendo $\underline{x}_1, \ldots, \underline{x}_n$, $n$ valores concretos de las variables independientes $(X_1,\ldots, X_k)$.

Fijarse que si la hipótesis nula $H_0$ es cierta, es decir $\beta_1=\cdots =\beta_k=0$, entonces los valores medios de las variables anteriores serían los mismos ya que recordemos que suponemos que:
$$
\mu_{Y|\underline{x}}=\beta_0 +\beta_1 \cdot x_1 +\cdots +\beta_k\cdot x_k=\beta_0,
$$
para todo valor de $\underline{x}$.

## Contrastes de hipótesis sobre los parámetros $\beta_i$
Por tanto, el contraste original sería equivalente a realizar el contraste **ANOVA** siguiente:
$$
\left.\begin{array}{l}
H_0:\mu_{Y|\underline{x}_1}=\cdots=\mu_{Y|\underline{x}_n},\\
 H_1:\mbox{existen $i$ y $j$ tales que }\mu_{Y|\underline{x}_i}\neq \mu_{Y|\underline{x}_j}.
\end{array}
\right\}
$$
Para que el modelo de **regresión lineal múltiple** tenga sentido hemos de rechazar la hipótesis nula anterior.

## Contrastes de hipótesis sobre los parámetros $\beta_i$
La **tabla ANOVA** es la siguiente:

<div class="center">
|Origen variación|Grados de libertad|Sumas de cuadrados | Cuadrados medios|Estadístico de contraste|p-valor|
|:---|:---:|:---:|:---:|:---:|:---:|
|Regresión|$k$|$SS_R$|$MS_R=\frac{SS_R}{k}$|$F=\frac{MS_R}{MS_E}$|p-valor|
|Error|$n-k-1$|$SS_E$|$MS_E=\frac{SS_E}{n-k-1}$| | |
</div>
donde el **estadístico de contraste** $F=\frac{MS_R}{MS_E}$, suponiendo la hipótesis nula cierta, sigue la distribución $F$ de $k$ y $n-k-1$ grados de libertad.

El p-valor del contraste anterior vale $p=P(F_{k,n-k-1}\geq f_0),$
siendo $f_0$ el valor del estadístico de contraste $F$ para nuestros datos.

## Tabla ANOVA en `R`
Para calcular la tabla ANOVA en `R` de una **regresión lineal múltiple** hay que usar la función `anova` de la forma siguiente:

```{r,eval=FALSE}
anova(lm(y ~ Xd))
```
donde `Xd` es una matrix cuyas columnas son los valores de las variables independientes $x_1,\ldots, x_k$.



## Ejemplo
<div class="example">
Para hallar la tabla ANOVA para los datos de nuestro ejemplo, hemos de hacer lo siguiente:
```{r}
anova(lm(y~X[,2:5]))
```
Observamos que el p-valor es muy pequeño. Por tanto, rechazamos la hipótesis nula y concluimos que la regresión ha tenido sentido en este caso.
</div>


## Contrastes de hipótesis sobre los parámetros $\beta_i$
Otro tipo de contrastes que podemos plantearnos sobre los parámetros del modelo $\beta_i$ es, si fijado $i$, $\beta_i$ aporta algo al modelo, o, si la variable $x_i$ es **significativa**.

Concretamente, fijado $i$, nos planteamos el contraste siguiente:
$$
\left.\begin{array}{l} H_0: \beta_i=0, \\
H_1: \beta_i \neq 0, \end{array}
\right\}
$$
usando como **estadístico de contraste** $\frac{\beta_i-b_i}{\sqrt{(S^2\cdot (\mathbf{X}^\top \mathbf{X})^{-1})_{ii}}}$,
que sabemos que sigue una ley $t$ de Student con $n-k-1$ grados de libertad. 

## Contrastes de hipótesis sobre los parámetros $\beta_i$

El p-valor del contraste anterior vale $p=2\cdot P(t_{n-k-1}>|t_0$, donde $t_0$ es el valor obtenido por el estadístico de contraste usando nuestros datos.

Para que la variable $x_i$ sea **significativa** o para que aporte información relevante al modelo de **regresión lineal múltiple**, debemos rechazar la hipótesis nula en el contraste anterior u obtener un p-valor pequeño.

## Contrastes de hipótesis sobre los parámetros $\beta_i$ en `R`
Para realizar todos los contrastes anteriores en `R` hay que usar la función `summary` aplicada al objecto `lm(...)`:
```{r,eval=FALSE}
summary(lm(y~x1+...+xk))
```
y `R` nos da toda la información sobre la **regresión múltiple** realizada.

## Ejemplo
<div class="example">
Si aplicamos la función `summary` a los datos de nuestro ejemplo, obtenemos la salida siguiente:

```{r,eval=FALSE}
summary(lm(y~x1+x2+x3+x4))
```

</div>

## Ejemplo

<div class="example">
```{r,echo=FALSE}
summary(lm(y~x1+x2+x3+x4))
```
</div>


## Ejemplo
<div class="example">
En primer lugar `R` nos da la lista de los errores cometidos en las estimaciones.

En segundo lugar, `R` nos muestra una tabla con las columnas siguientes:

* `Estimate`: los valores estimados de los parámetros $\beta_0,\beta_1,\beta_2,\beta_3$ y $\beta_4$. Es decir, los valores $b_0,b_1,b_2,b_3$ y $b_4$.
* `Std. Error`: los errores estándars de los estimadores $b_0,b_1,b_2,b_3$ y $b_4$.
* `t value`: el valor del estadístico de contraste cuando realizamos el contraste $\left.\begin{array}{l} H_0: \beta_i=0, \\
H_1: \beta_i \neq 0, \end{array}
\right\}$ sobre cada parámetro $\beta_i$, $i=0,1,2,3,4$.
* `Pr(>|t|)`: los p-valores del contraste anterior.

Observamos que todas las variables excepto la variable `x3`, peso del niño al nacer, son no significativas para el modelo.

A continuación nos da el error residual que es la estimación de $\sqrt{S^2}$, el valor de coeficiente de determinación $R^2$ y el coeficiente de determinación ajustado $R^2_{adj}$.

Para finalizar nos da el valor del estadístico de contraste ANOVA comentado anteriormente junto con el p-valor del mismo.

</div>
